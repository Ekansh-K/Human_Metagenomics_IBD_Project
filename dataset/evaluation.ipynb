{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Score\n",
    "Calculate and display the accuracy score of the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Score\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(\"Accuracy Score: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "Generate and display the confusion matrix to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# Generate and display the confusion matrix\n",
    "conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "print(\"Confusion Matrix: \\n\", conf_matrix)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(modelKNNC, X_test, Y_test, cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision, Recall, and F1-Score\n",
    "Calculate and display precision, recall, and F1-score to provide a detailed evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, Recall, and F1-Score\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(Y_test, Y_pred, average='weighted')\n",
    "print(\"Precision: \", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(Y_test, Y_pred, average='weighted')\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(Y_test, Y_pred, average='weighted')\n",
    "print(\"F1-Score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve and AUC\n",
    "Plot the ROC curve and calculate the AUC to evaluate the model's ability to distinguish between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr, tpr, _ = roc_curve(Y_test, modelKNNC.predict_proba(X_test)[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Print AUC\n",
    "print(\"AUC: \", roc_auc)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
